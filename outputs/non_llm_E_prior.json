[
  {
    "node_i": 40,
    "node_j": 179,
    "node_i_label": "2016",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.833398824930191,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 175,
    "node_j": 179,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8262183904647827,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 166,
    "node_j": 179,
    "node_i_label": "Mu Li",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8196200013160706,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 168,
    "node_j": 179,
    "node_i_label": "Dive",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8194134771823883,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Natural Language Processing with Transformers.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 176,
    "node_j": 179,
    "node_i_label": "Sebastian Raschka",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8190752506256104,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 169,
    "node_j": 179,
    "node_i_label": "\u25aa Daniel Graupe",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.817382988333702,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 172,
    "node_j": 179,
    "node_i_label": "Lewis Tunstall",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8161213129758835,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 167,
    "node_j": 179,
    "node_i_label": "Alexander J. Smola",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8143320918083191,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 40,
    "node_j": 175,
    "node_i_label": "2016",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.8132304191589356,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 26,
    "node_j": 90,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Doc A. o",
    "support": 0.8114658236503601,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 177,
    "node_j": 179,
    "node_i_label": "Manning Publications",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8102134168148041,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 169,
    "node_j": 175,
    "node_i_label": "\u25aa Daniel Graupe",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.8062207221984863,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 175,
    "node_j": 182,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "2024",
    "support": 0.80425665974617,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 166,
    "node_j": 175,
    "node_i_label": "Mu Li",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.803906774520874,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 26,
    "node_j": 85,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Doc",
    "support": 0.8016931921243667,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 175,
    "node_j": 178,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "2025",
    "support": 0.7992033720016479,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 163,
    "node_j": 179,
    "node_i_label": "MIT Press",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7969716906547546,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 159,
    "node_j": 175,
    "node_i_label": "Text Book",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7965311646461487,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 39,
    "node_j": 175,
    "node_i_label": "Deep Learning",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7960976302623749,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016."
    ]
  },
  {
    "node_i": 26,
    "node_j": 34,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Model Docs",
    "support": 0.796079158782959,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)"
    ]
  },
  {
    "node_i": 26,
    "node_j": 89,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "\u25aaRetriever",
    "support": 0.7959393918514251,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 173,
    "node_j": 175,
    "node_i_label": "Leandro von Werra",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7955900728702545,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 26,
    "node_j": 82,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "REALM \u25aa Pre-training Objective",
    "support": 0.7936756491661072,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 162,
    "node_j": 179,
    "node_i_label": "Aaron Courville",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7934474408626556,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 172,
    "node_j": 175,
    "node_i_label": "Lewis Tunstall",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7934123814105988,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 173,
    "node_j": 179,
    "node_i_label": "Leandro von Werra",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7933485507965088,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf."
    ]
  },
  {
    "node_i": 174,
    "node_j": 179,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7927880913019181,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 175,
    "node_j": 176,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Sebastian Raschka",
    "support": 0.7924245297908783,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 26,
    "node_j": 205,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.7922083646059036,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ]
  },
  {
    "node_i": 161,
    "node_j": 179,
    "node_i_label": "Yoshua Bengio",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7905851066112518,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 26,
    "node_j": 208,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "\u25aaWould",
    "support": 0.7894416004419327,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 167,
    "node_j": 175,
    "node_i_label": "Alexander J. Smola",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.788301819562912,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 171,
    "node_j": 179,
    "node_i_label": "World Scientific Publishing Co., Inc.",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7789101541042328,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 168,
    "node_j": 175,
    "node_i_label": "Dive",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7789084017276764,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Natural Language Processing with Transformers.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 130,
    "node_j": 183,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Overview",
    "support": 0.7770897001028061,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 26,
    "node_j": 83,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "P(y|x",
    "support": 0.7765065789222717,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 26,
    "node_j": 28,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "P(y",
    "support": 0.7757582813501358,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 26,
    "node_j": 46,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "p(y",
    "support": 0.7757582813501358,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 130,
    "node_j": 193,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Learning",
    "support": 0.7756351351737976,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 26,
    "node_j": 204,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "13",
    "support": 0.7730366975069046,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 174,
    "node_j": 175,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7714716017246246,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 161,
    "node_j": 175,
    "node_i_label": "Yoshua Bengio",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7696829319000245,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 26,
    "node_j": 48,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "q=",
    "support": 0.765222242474556,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 26,
    "node_j": 91,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "28",
    "support": 0.7640379518270493,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 130,
    "node_j": 196,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7633843809366226,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 130,
    "node_j": 192,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Strategies-II \u2022",
    "support": 0.7630441814661026,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 184,
    "node_j": 196,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7630114018917084,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 130,
    "node_j": 197,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7625804752111435,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 14,
    "node_j": 194,
    "node_i_label": "NLP",
    "node_j_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "support": 0.7602871894836426,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 26,
    "node_j": 206,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Memory Requirement",
    "support": 0.7601301044225692,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 130,
    "node_j": 198,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7596180737018585,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 175,
    "node_j": 181,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Wiley Publications",
    "support": 0.7588085919618607,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 33,
    "node_j": 130,
    "node_i_label": "Train Knowledge Time = (Be boys - on -",
    "node_j_label": "IIIT BANGALORE 7 Retrieval",
    "support": 0.7587016016244889,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 14,
    "node_j": 196,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7550015449523926,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Natural Language Processing with Transformers.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task"
    ]
  },
  {
    "node_i": 163,
    "node_j": 175,
    "node_i_label": "MIT Press",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7549911946058273,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "# params BERT-Baseline (Lee et al., 2019)"
    ]
  },
  {
    "node_i": 175,
    "node_j": 180,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Tanmoy Chakraborty",
    "support": 0.7535027623176574,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 130,
    "node_j": 187,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Multi",
    "support": 0.7529729008674622,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 188,
    "node_j": 196,
    "node_i_label": "\u2022 Fine-tuning",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7525039047002793,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 130,
    "node_j": 195,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7515892952680587,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 183,
    "node_j": 196,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7511678040027618,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 195,
    "node_j": 196,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7501694053411484,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @"
    ]
  },
  {
    "node_i": 175,
    "node_j": 177,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Manning Publications",
    "support": 0.7495678544044495,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g"
    ]
  },
  {
    "node_i": 14,
    "node_j": 191,
    "node_i_label": "NLP",
    "node_j_label": "Advanced Post-training",
    "support": 0.7468766897916794,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 193,
    "node_j": 196,
    "node_i_label": "Learning",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7462854951620101,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 187,
    "node_j": 196,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7452965795993804,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task"
    ]
  },
  {
    "node_i": 78,
    "node_j": 80,
    "node_i_label": "Document Encoder",
    "node_j_label": "Reader \u2013 LM \u25aaPre",
    "support": 0.7439376294612885,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Input LM ey >| | Output",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 141,
    "node_j": 143,
    "node_i_label": "SAHA",
    "node_j_label": "IIIT BANGALORE 1 Slides",
    "support": 0.7437904715538025,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA, IIIT BANGALORE 28",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 196,
    "node_j": 201,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "4",
    "support": 0.7424324780702591,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 78,
    "node_j": 170,
    "node_i_label": "Document Encoder",
    "node_j_label": "Deep Learning Neural Networks: Design and Case Studies",
    "support": 0.7419797390699386,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 196,
    "node_j": 200,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "RLHF",
    "support": 0.7391494333744049,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 196,
    "node_j": 198,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7388936668634415,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 77,
    "node_j": 80,
    "node_i_label": "Reader & Retriever \u25aaTrainable Components",
    "node_j_label": "Reader \u2013 LM \u25aaPre",
    "support": 0.7377566635608673,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 194,
    "node_j": 199,
    "node_i_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "node_j_label": "the Loop Feedback",
    "support": 0.7373454660177231,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 39,
    "node_j": 80,
    "node_i_label": "Deep Learning",
    "node_j_label": "Reader \u2013 LM \u25aaPre",
    "support": 0.737143161892891,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 14,
    "node_j": 193,
    "node_i_label": "NLP",
    "node_j_label": "Learning",
    "support": 0.7362256199121475,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 191,
    "node_j": 196,
    "node_i_label": "Advanced Post-training",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7357051730155945,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 33,
    "node_j": 196,
    "node_i_label": "Train Knowledge Time = (Be boys - on -",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7341985315084457,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 143,
    "node_j": 149,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "P. Bhattacharyya",
    "support": 0.7337179243564605,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "= log P(y \u2223 x, z) TULIKA SAHA, IIIT BANGALORE 27",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "TULIKA SAHA, IIIT BANGALORE 28",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 26,
    "node_j": 42,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Training Ci Contexts Targets Ui",
    "support": 0.73368239402771,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:"
    ]
  },
  {
    "node_i": 26,
    "node_j": 45,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Hawaii Hawaii",
    "support": 0.7318074852228165,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Hawaii | 0.6",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 192,
    "node_j": 196,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7316198259592056,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 34,
    "node_j": 80,
    "node_i_label": "Model Docs",
    "node_j_label": "Reader \u2013 LM \u25aaPre",
    "support": 0.7315373808145523,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 39,
    "node_j": 168,
    "node_i_label": "Deep Learning",
    "node_j_label": "Dive",
    "support": 0.7314030021429062,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 196,
    "node_j": 197,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7307589679956437,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ]
  },
  {
    "node_i": 78,
    "node_j": 205,
    "node_i_label": "Document Encoder",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.7303448796272278,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)"
    ]
  },
  {
    "node_i": 196,
    "node_j": 199,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "the Loop Feedback",
    "support": 0.7292798340320588,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task"
    ]
  },
  {
    "node_i": 14,
    "node_j": 195,
    "node_i_label": "NLP",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7290746450424195,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 39,
    "node_j": 78,
    "node_i_label": "Deep Learning",
    "node_j_label": "Document Encoder",
    "support": 0.7281633526086807,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016."
    ]
  },
  {
    "node_i": 14,
    "node_j": 188,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Fine-tuning",
    "support": 0.7277314841747284,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 171,
    "node_j": 175,
    "node_i_label": "World Scientific Publishing Co., Inc.",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7275865554809571,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 80,
    "node_j": 207,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "Computational Cost",
    "support": 0.7272527307271958,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 14,
    "node_j": 183,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Overview",
    "support": 0.7266071617603302,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 82,
    "node_j": 85,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "Doc",
    "support": 0.7257860004901886,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 14,
    "node_j": 198,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.724541375041008,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Natural Language Processing with Transformers.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 197,
    "node_i_label": "NLP",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7236123949289321,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)"
    ]
  },
  {
    "node_i": 39,
    "node_j": 177,
    "node_i_label": "Deep Learning",
    "node_j_label": "Manning Publications",
    "support": 0.7232302099466323,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 143,
    "node_j": 144,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "Stanford",
    "support": 0.7216133296489715,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Columbia University.|University.",
      "Columbia University.|University."
    ]
  },
  {
    "node_i": 198,
    "node_j": 199,
    "node_i_label": "\u2022 Policy Learning",
    "node_j_label": "the Loop Feedback",
    "support": 0.7205691039562225,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 78,
    "node_j": 168,
    "node_i_label": "Document Encoder",
    "node_j_label": "Dive",
    "support": 0.7194530427455902,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 193,
    "node_j": 195,
    "node_i_label": "Learning",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7188612341880798,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 187,
    "node_j": 198,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7178585410118103,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 185,
    "node_j": 194,
    "node_i_label": "GPT",
    "node_j_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "support": 0.7173742145299912,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 195,
    "node_j": 198,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7173502713441848,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 183,
    "node_j": 198,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7152105331420898,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 78,
    "node_j": 206,
    "node_i_label": "Document Encoder",
    "node_j_label": "Memory Requirement",
    "support": 0.7143559098243714,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 79,
    "node_j": 205,
    "node_i_label": "Query Encoder",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.7137590050697327,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Indexed Contexts (keys) ...",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 78,
    "node_j": 208,
    "node_i_label": "Document Encoder",
    "node_j_label": "\u25aaWould",
    "support": 0.7114558428525924,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)"
    ]
  },
  {
    "node_i": 34,
    "node_j": 35,
    "node_i_label": "Model Docs",
    "node_j_label": "Open",
    "support": 0.7110890924930573,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d"
    ]
  },
  {
    "node_i": 185,
    "node_j": 196,
    "node_i_label": "GPT",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7104750484228134,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 90,
    "node_j": 206,
    "node_i_label": "Doc A. o",
    "node_j_label": "Memory Requirement",
    "support": 0.7099889844655991,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 14,
    "node_j": 192,
    "node_i_label": "NLP",
    "node_j_label": "Strategies-II \u2022",
    "support": 0.709892001748085,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Natural Language Processing with Transformers.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 193,
    "node_j": 199,
    "node_i_label": "Learning",
    "node_j_label": "the Loop Feedback",
    "support": 0.7092069864273072,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 39,
    "node_j": 181,
    "node_i_label": "Deep Learning",
    "node_j_label": "Wiley Publications",
    "support": 0.7091768175363541,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 195,
    "node_j": 197,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7090806066989899,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 14,
    "node_j": 187,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Multi",
    "support": 0.7089264839887619,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 187,
    "node_j": 197,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7088432520627975,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 79,
    "node_j": 206,
    "node_i_label": "Query Encoder",
    "node_j_label": "Memory Requirement",
    "support": 0.7083703368902207,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 48,
    "node_j": 82,
    "node_i_label": "q=",
    "node_j_label": "REALM \u25aa Pre-training Objective",
    "support": 0.7080561339855194,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 82,
    "node_j": 204,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "13",
    "support": 0.7078821897506714,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 187,
    "node_j": 193,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "Learning",
    "support": 0.7070366352796554,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 14,
    "node_j": 189,
    "node_i_label": "NLP",
    "node_j_label": "Efficient Fine",
    "support": 0.7068032264709473,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 183,
    "node_j": 193,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Learning",
    "support": 0.7060031712055206,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 78,
    "node_j": 207,
    "node_i_label": "Document Encoder",
    "node_j_label": "Computational Cost",
    "support": 0.7046526998281479,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 191,
    "node_j": 195,
    "node_i_label": "Advanced Post-training",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7037585437297821,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 3,
    "node_j": 9,
    "node_i_label": "IIIT BANGALORE",
    "node_j_label": "Test Context Target Representation",
    "support": 0.7030752837657929,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "=Apaun(y)+(1\u2014A}pu pty) ty) Test Context Target Representation x q= q= _ f(z)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 197,
    "node_j": 198,
    "node_i_label": "Reinforcement Learning",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7028891324996949,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 193,
    "node_j": 198,
    "node_i_label": "Learning",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7016630530357361,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 14,
    "node_j": 190,
    "node_i_label": "NLP",
    "node_j_label": "Mandate 3:",
    "support": 0.7013584613800049,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Natural Language Processing with Transformers.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 82,
    "node_j": 87,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "70%",
    "support": 0.7009221285581588,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 39,
    "node_j": 166,
    "node_i_label": "Deep Learning",
    "node_j_label": "Mu Li",
    "support": 0.6988522440195084,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 80,
    "node_j": 168,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "Dive",
    "support": 0.6987922668457032,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 82,
    "node_j": 88,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "5%",
    "support": 0.6983321249485016,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ]
  },
  {
    "node_i": 193,
    "node_j": 197,
    "node_i_label": "Learning",
    "node_j_label": "Reinforcement Learning",
    "support": 0.6981014043092728,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 184,
    "node_j": 195,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.698030436038971,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 80,
    "node_j": 160,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "Ian Goodfellow",
    "support": 0.6973617404699326,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "How to use the Book? /",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 39,
    "node_j": 171,
    "node_i_label": "Deep Learning",
    "node_j_label": "World Scientific Publishing Co., Inc.",
    "support": 0.6962847441434861,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 14,
    "node_j": 201,
    "node_i_label": "NLP",
    "node_j_label": "4",
    "support": 0.6959917634725571,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Natural Language Processing with Transformers.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 184,
    "node_j": 197,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "Reinforcement Learning",
    "support": 0.695552545785904,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 188,
    "node_j": 195,
    "node_i_label": "\u2022 Fine-tuning",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6950279891490936,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 13 \u25aaLimitations:"
    ]
  },
  {
    "node_i": 193,
    "node_j": 200,
    "node_i_label": "Learning",
    "node_j_label": "RLHF",
    "support": 0.6946500182151795,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 34,
    "node_j": 85,
    "node_i_label": "Model Docs",
    "node_j_label": "Doc",
    "support": 0.6943202644586564,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)"
    ]
  },
  {
    "node_i": 143,
    "node_j": 146,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "UMass",
    "support": 0.6932398855686188,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "Columbia University.|University."
    ]
  },
  {
    "node_i": 34,
    "node_j": 89,
    "node_i_label": "Model Docs",
    "node_j_label": "\u25aaRetriever",
    "support": 0.6924109995365143,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)"
    ]
  },
  {
    "node_i": 192,
    "node_j": 197,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "Reinforcement Learning",
    "support": 0.692205473780632,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 143,
    "node_j": 160,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "Ian Goodfellow",
    "support": 0.6915785372257233,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs"
    ]
  },
  {
    "node_i": 186,
    "node_j": 191,
    "node_i_label": "Mandate 2:",
    "node_j_label": "Advanced Post-training",
    "support": 0.6913096725940704,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 79,
    "node_j": 207,
    "node_i_label": "Query Encoder",
    "node_j_label": "Computational Cost",
    "support": 0.6910345554351807,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ]
  },
  {
    "node_i": 206,
    "node_j": 207,
    "node_i_label": "Memory Requirement",
    "node_j_label": "Computational Cost",
    "support": 0.6910264104604721,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ]
  },
  {
    "node_i": 184,
    "node_j": 198,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.6904240101575851,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:"
    ]
  },
  {
    "node_i": 190,
    "node_j": 191,
    "node_i_label": "Mandate 3:",
    "node_j_label": "Advanced Post-training",
    "support": 0.6900470286607743,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 205,
    "node_j": 207,
    "node_i_label": "\u25aaLimitations:",
    "node_j_label": "Computational Cost",
    "support": 0.6899703353643417,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 159,
    "node_j": 166,
    "node_i_label": "Text Book",
    "node_j_label": "Mu Li",
    "support": 0.6885635465383529,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "How to use the Book? /",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 14,
    "node_j": 185,
    "node_i_label": "NLP",
    "node_j_label": "GPT",
    "support": 0.6873183190822602,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "= log P(y \u2223 x, z) \u25aaFor example, o If Doc A retrieved \u2192 reader predicts [MASK] with 95% confidence --> R(A) = log(0.95)"
    ]
  },
  {
    "node_i": 189,
    "node_j": 195,
    "node_i_label": "Efficient Fine",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6863542199134827,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 190,
    "node_j": 198,
    "node_i_label": "Mandate 3:",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.685569766163826,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 90,
    "node_j": 158,
    "node_i_label": "Doc A. o",
    "node_j_label": "10%",
    "support": 0.6854154080152511,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d"
    ]
  },
  {
    "node_i": 185,
    "node_j": 198,
    "node_i_label": "GPT",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.684608793258667,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 146,
    "node_j": 159,
    "node_i_label": "UMass",
    "node_j_label": "Text Book",
    "support": 0.6840496957302094,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "How to use the Book? /",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 192,
    "node_j": 193,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "Learning",
    "support": 0.683394393324852,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "How to use the Book? /"
    ]
  },
  {
    "node_i": 187,
    "node_j": 195,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6833462625741958,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d"
    ]
  },
  {
    "node_i": 14,
    "node_j": 41,
    "node_i_label": "NLP",
    "node_j_label": "Quantization",
    "support": 0.6828564673662185,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Natural Language Processing with Transformers.",
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 85,
    "node_j": 86,
    "node_i_label": "Doc",
    "node_j_label": "95%",
    "support": 0.6815871685743332,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "DR."
    ]
  },
  {
    "node_i": 85,
    "node_j": 88,
    "node_i_label": "Doc",
    "node_j_label": "5%",
    "support": 0.6813487529754638,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "# params BERT-Baseline (Lee et al., 2019)"
    ]
  },
  {
    "node_i": 28,
    "node_j": 42,
    "node_i_label": "P(y",
    "node_j_label": "Training Ci Contexts Targets Ui",
    "support": 0.681280642747879,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "=Apaun(y)+(1\u2014A}pu pty) ty) Test Context Target Representation x q= q= _ f(z)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 14,
    "node_j": 199,
    "node_i_label": "NLP",
    "node_j_label": "the Loop Feedback",
    "support": 0.6811994045972825,
    "temporal": 0.0,
    "mechanistic": 0.8,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 183,
    "node_j": 195,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6807333558797837,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 41,
    "node_j": 193,
    "node_i_label": "Quantization",
    "node_j_label": "Learning",
    "support": 0.6800093799829483,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 186,
    "node_j": 198,
    "node_i_label": "Mandate 2:",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.6799672052264214,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 88,
    "node_j": 90,
    "node_i_label": "5%",
    "node_j_label": "Doc A. o",
    "support": 0.679862454533577,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "DR."
    ]
  },
  {
    "node_i": 90,
    "node_j": 154,
    "node_i_label": "Doc A. o",
    "node_j_label": "30%",
    "support": 0.6789451092481613,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 207,
    "node_j": 208,
    "node_i_label": "Computational Cost",
    "node_j_label": "\u25aaWould",
    "support": 0.6786350280046463,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 197,
    "node_j": 199,
    "node_i_label": "Reinforcement Learning",
    "node_j_label": "the Loop Feedback",
    "support": 0.6779278129339218,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 141,
    "node_j": 144,
    "node_i_label": "SAHA",
    "node_j_label": "Stanford",
    "support": 0.6768651187419892,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "University.| University.",
      "Obama was born in Hawaii, and graduated from Obama| was Columbia University. ...",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "TULIKA SAHA, IIIT BANGALORE 28"
    ]
  },
  {
    "node_i": 79,
    "node_j": 168,
    "node_i_label": "Query Encoder",
    "node_j_label": "Dive",
    "support": 0.6759675323963166,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Indexed Contexts (keys) ..."
    ]
  },
  {
    "node_i": 191,
    "node_j": 199,
    "node_i_label": "Advanced Post-training",
    "node_j_label": "the Loop Feedback",
    "support": 0.6745476275682449,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 185,
    "node_j": 193,
    "node_i_label": "GPT",
    "node_j_label": "Learning",
    "support": 0.6737821847200394,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 79,
    "node_j": 208,
    "node_i_label": "Query Encoder",
    "node_j_label": "\u25aaWould",
    "support": 0.6731336086988449,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)"
    ]
  },
  {
    "node_i": 11,
    "node_j": 34,
    "node_i_label": "3",
    "node_j_label": "Model Docs",
    "support": 0.6731049567461014,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "# params BERT-Baseline (Lee et al., 2019)"
    ]
  },
  {
    "node_i": 143,
    "node_j": 165,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "Zachary C. Lipton",
    "support": 0.6714861512184143,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 143,
    "node_j": 145,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "C. Manning",
    "support": 0.6710103958845138,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "TULIKA SAHA, IIIT BANGALORE 28",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 85,
    "node_j": 87,
    "node_i_label": "Doc",
    "node_j_label": "70%",
    "support": 0.6699764519929886,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "DR.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 34,
    "node_j": 86,
    "node_i_label": "Model Docs",
    "node_j_label": "95%",
    "support": 0.6696989566087723,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 192,
    "node_j": 195,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6689423143863678,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @"
    ]
  },
  {
    "node_i": 132,
    "node_j": 153,
    "node_i_label": "Modify the LM",
    "node_j_label": "Exam & Quiz",
    "support": 0.6684932023286819,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Input LM ey >| | Output",
      "For e.g., kNN-LMs \u25aa Intermediate Fusion o Modify the LM architecture to be aware of the book o For e.g., ToolFormer, FLARE, RETRO etc. \u25aa Input augmentation (RAG)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 135,
    "node_j": 153,
    "node_i_label": "RAG",
    "node_j_label": "Exam & Quiz",
    "support": 0.6664598152041435,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "o Before you start solving o For e.g., RAG, REALM etc.",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "How to use the Book? /",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 187,
    "node_j": 199,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "the Loop Feedback",
    "support": 0.6657453447580337,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?",
      "Transformer Seq2Seq TS (Multitask) 29.8 32.2 - 738m TS (11b) (Roberts et al., 2020)"
    ]
  },
  {
    "node_i": 192,
    "node_j": 199,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "the Loop Feedback",
    "support": 0.6650861591100693,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @"
    ]
  },
  {
    "node_i": 195,
    "node_j": 199,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "the Loop Feedback",
    "support": 0.6650136500597,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 13 \u25aaLimitations:"
    ]
  },
  {
    "node_i": 34,
    "node_j": 88,
    "node_i_label": "Model Docs",
    "node_j_label": "5%",
    "support": 0.664110979437828,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 174,
    "node_j": 181,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "Wiley Publications",
    "support": 0.6630848303437233,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to use the Book? /",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g"
    ]
  },
  {
    "node_i": 188,
    "node_j": 189,
    "node_i_label": "\u2022 Fine-tuning",
    "node_j_label": "Efficient Fine",
    "support": 0.6611572533845902,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "o Memory Requirement o Computational Cost o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 190,
    "node_j": 195,
    "node_i_label": "Mandate 3:",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6608657702803612,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Indexed Contexts (keys) ...",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:"
    ]
  },
  {
    "node_i": 34,
    "node_j": 87,
    "node_i_label": "Model Docs",
    "node_j_label": "70%",
    "support": 0.6603366762399674,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 105,
    "node_j": 122,
    "node_i_label": "11b",
    "node_j_label": "Wikipedia",
    "support": 0.6603322178125381,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "+ APinn O12)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 183,
    "node_j": 199,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "the Loop Feedback",
    "support": 0.6600900888442993,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ]
  },
  {
    "node_i": 54,
    "node_j": 66,
    "node_i_label": "billions",
    "node_j_label": "256 1024",
    "support": 0.6582235455513,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "o Memory Requirement o Computational Cost o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Dense Retr.+-Transformer REALM 40.4 40.7 42.9 330m"
    ]
  },
  {
    "node_i": 4,
    "node_j": 122,
    "node_i_label": "al.",
    "node_j_label": "Wikipedia",
    "support": 0.6570942223072052,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Ilinois | 0.2 Obama's birthplace is OOOO)"
    ]
  },
  {
    "node_i": 183,
    "node_j": 186,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Mandate 2:",
    "support": 0.6564009040594101,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 186,
    "node_j": 195,
    "node_i_label": "Mandate 2:",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6537778869271278,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Indexed Contexts (keys) ..."
    ]
  },
  {
    "node_i": 209,
    "node_j": 216,
    "node_i_label": "trillions",
    "node_j_label": "\u2022 How",
    "support": 0.6529964163899422,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Aggregation = = Se S\u2014 lyenpth)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)"
    ]
  },
  {
    "node_i": 183,
    "node_j": 190,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Mandate 3:",
    "support": 0.6526997208595275,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 176,
    "node_j": 181,
    "node_i_label": "Sebastian Raschka",
    "node_j_label": "Wiley Publications",
    "support": 0.6516284048557281,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to use the Book? /",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o"
    ]
  },
  {
    "node_i": 195,
    "node_j": 200,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "RLHF",
    "support": 0.6476932182908058,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Indexed Contexts (keys) ..."
    ]
  },
  {
    "node_i": 146,
    "node_j": 150,
    "node_i_label": "UMass",
    "node_j_label": "IIT-Delhi",
    "support": 0.6471543639898301,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Columbia University.|University.",
      "Cj Uy ki = F(ci) ... ..",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "TULIKA SAHA, IIIT BANGALORE 28",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 18,
    "node_j": 54,
    "node_i_label": "15",
    "node_j_label": "billions",
    "support": 0.6460234612226486,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "16.8 fever fevvneereeoneeeeeeee? 3.9 Poe C8 2 a ROO coe Py \u00bb a N a aa 1 2 8 64 256 1024 k (# nearest neighbors)"
    ]
  },
  {
    "node_i": 141,
    "node_j": 146,
    "node_i_label": "SAHA",
    "node_j_label": "UMass",
    "support": 0.6448932468891144,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA, IIIT BANGALORE 28",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "o Before you start solving o For e.g., RAG, REALM etc.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Obama was born in Hawaii, and graduated from Training Contexts Targets Representations Columbia University. ..."
    ]
  },
  {
    "node_i": 183,
    "node_j": 192,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Strategies-II \u2022",
    "support": 0.6399892166256904,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "How to use the Book? /",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 174,
    "node_j": 177,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "Manning Publications",
    "support": 0.6368502348661422,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "How to use the Book? /",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 172,
    "node_j": 177,
    "node_i_label": "Lewis Tunstall",
    "node_j_label": "Manning Publications",
    "support": 0.6361418917775155,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Indexed Keys (N Values (N,F Obama Obama _was was born born in|Obama in\\Obama was was born born in in Hawaii] Hawaii and|'and graduated from Columbia] University] land graduated fromland graduated from Columbia.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ]
  },
  {
    "node_i": 59,
    "node_j": 63,
    "node_i_label": "UB",
    "node_j_label": "ROO",
    "support": 0.6354605823755264,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "o Before you start solving o For e.g., RAG, REALM etc.",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 166,
    "node_j": 171,
    "node_i_label": "Mu Li",
    "node_j_label": "World Scientific Publishing Co., Inc.",
    "support": 0.6353148072957993,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 65,
    "node_j": 66,
    "node_i_label": "1 2",
    "node_j_label": "256 1024",
    "support": 0.6352824911475181,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "o Memory Requirement o Computational Cost o",
      "= (1- A)Pim =)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 50,
    "node_j": 54,
    "node_i_label": "5",
    "node_j_label": "billions",
    "support": 0.6315828323364258,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Pa 6",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 163,
    "node_j": 181,
    "node_i_label": "MIT Press",
    "node_j_label": "Wiley Publications",
    "support": 0.6273838594555855,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Columbia University.|University.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 185,
    "node_j": 195,
    "node_i_label": "GPT",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6237500041723252,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Indexed Contexts (keys) ...",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 20,
    "node_j": 122,
    "node_i_label": "Roberts",
    "node_j_label": "Wikipedia",
    "support": 0.6211097702383995,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Obama was born in Hawaii, and graduated from Obama| was Columbia University. ...",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o"
    ]
  },
  {
    "node_i": 37,
    "node_j": 177,
    "node_i_label": "borntin",
    "node_j_label": "Manning Publications",
    "support": 0.619210334122181,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "# params BERT-Baseline (Lee et al., 2019)",
      "Indexed Keys (N Values (N,F Obama Obama _was was born born in|Obama in\\Obama was was born born in in Hawaii] Hawaii and|'and graduated from Columbia] University] land graduated fromland graduated from Columbia.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf."
    ]
  },
  {
    "node_i": 202,
    "node_j": 210,
    "node_i_label": "11",
    "node_j_label": "2022",
    "support": 0.6149005815386772,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "+ APinn O12)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Hawaii |0.2",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)"
    ]
  },
  {
    "node_i": 177,
    "node_j": 180,
    "node_i_label": "Manning Publications",
    "node_j_label": "Tanmoy Chakraborty",
    "support": 0.6138563394546509,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "How to use the Book? /",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 166,
    "node_j": 177,
    "node_i_label": "Mu Li",
    "node_j_label": "Manning Publications",
    "support": 0.6098399981856346,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "# params BERT-Baseline (Lee et al., 2019)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Columbia University.|University."
    ]
  },
  {
    "node_i": 81,
    "node_j": 205,
    "node_i_label": "26",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.6095624923706054,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Pa 6",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ]
  },
  {
    "node_i": 161,
    "node_j": 163,
    "node_i_label": "Yoshua Bengio",
    "node_j_label": "MIT Press",
    "support": 0.6082770317792893,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 168,
    "node_j": 177,
    "node_i_label": "Dive",
    "node_j_label": "Manning Publications",
    "support": 0.6042516484856606,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Indexed Keys (N Values (N,F Obama Obama _was was born born in|Obama in\\Obama was was born born in in Hawaii] Hawaii and|'and graduated from Columbia] University] land graduated fromland graduated from Columbia.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  }
]