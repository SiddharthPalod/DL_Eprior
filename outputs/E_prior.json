[
  {
    "node_i": 95,
    "node_j": 114,
    "node_i_label": "BERT-Baseline",
    "node_j_label": "31.8",
    "support": 0.9,
    "temporal": 0.5,
    "mechanistic": 0.2,
    "contexts": [
      "# params BERT-Baseline (Lee et al., 2019)",
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "Sparse Retr.4-Transformer BERT 26.5 17.7 21.3 110m TS (base) (Roberts et al., 2020)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)"
    ],
    "rationale": "The evidence shows that BERT-Baseline has a value of 31.8 in some context, suggesting a direct relationship."
  },
  {
    "node_i": 185,
    "node_j": 193,
    "node_i_label": "GPT",
    "node_j_label": "Learning",
    "support": 0.9,
    "temporal": 0.7,
    "mechanistic": 0.6,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ],
    "rationale": "The evidence suggests that GPT models are used in pre-training and fine-tuning strategies, which are essential components of the learning process for these models."
  },
  {
    "node_i": 185,
    "node_j": 188,
    "node_i_label": "GPT",
    "node_j_label": "\u2022 Fine-tuning",
    "support": 0.9,
    "temporal": 0.7,
    "mechanistic": 0.6,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 13 \u25aaLimitations:",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ],
    "rationale": "The evidence suggests that GPT models are subject to fine-tuning as a post-training strategy."
  },
  {
    "node_i": 85,
    "node_j": 89,
    "node_i_label": "Doc",
    "node_j_label": "\u25aaRetriever",
    "support": 0.9,
    "temporal": 0.7,
    "mechanistic": 0.6,
    "contexts": [
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "DR.",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)"
    ],
    "rationale": "The retriever learns to fetch documents based on the reward it yields, suggesting a causal relationship where the retriever's behavior is influenced by the document's properties."
  },
  {
    "node_i": 40,
    "node_j": 179,
    "node_i_label": "2016",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.833398824930191,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 179,
    "node_j": 182,
    "node_i_label": "\u25aa Introduction to Large Language Models",
    "node_j_label": "2024",
    "support": 0.828096741437912,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 175,
    "node_j": 179,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8262183904647827,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 166,
    "node_j": 179,
    "node_i_label": "Mu Li",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8196200013160706,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 168,
    "node_j": 179,
    "node_i_label": "Dive",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8194134771823883,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Natural Language Processing with Transformers.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 176,
    "node_j": 179,
    "node_i_label": "Sebastian Raschka",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8190752506256104,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 169,
    "node_j": 179,
    "node_i_label": "\u25aa Daniel Graupe",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.817382988333702,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 167,
    "node_j": 179,
    "node_i_label": "Alexander J. Smola",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8143320918083191,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 40,
    "node_j": 175,
    "node_i_label": "2016",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.8132304191589356,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 26,
    "node_j": 90,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Doc A. o",
    "support": 0.8114658236503601,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 177,
    "node_j": 179,
    "node_i_label": "Manning Publications",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.8102134168148041,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 169,
    "node_j": 175,
    "node_i_label": "\u25aa Daniel Graupe",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.8062207221984863,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 175,
    "node_j": 182,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "2024",
    "support": 0.80425665974617,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 166,
    "node_j": 175,
    "node_i_label": "Mu Li",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.803906774520874,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 26,
    "node_j": 85,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Doc",
    "support": 0.8016931921243667,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 30,
    "node_j": 119,
    "node_i_label": "Lee et al.",
    "node_j_label": "ORQA",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.2,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Obama was] senator ...",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "# params BERT-Baseline (Lee et al., 2019)"
    ],
    "rationale": "Lee et al. is mentioned as a baseline in the context of ORQA, suggesting a potential influence or comparison."
  },
  {
    "node_i": 133,
    "node_j": 134,
    "node_i_label": "ToolFormer",
    "node_j_label": "FLARE",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "For e.g., kNN-LMs \u25aa Intermediate Fusion o Modify the LM architecture to be aware of the book o For e.g., ToolFormer, FLARE, RETRO etc. \u25aa Input augmentation (RAG)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Parametric distribution",
      "Sparse Retr.4-Transformer BERT 28.1 - - 110m GraphRetriever (Min et al., 2019b) GraphRetriever+-Transformer"
    ],
    "rationale": "Evidence 1 mentions ToolFormer and FLARE as examples, suggesting a relationship, possibly of influence or similarity."
  },
  {
    "node_i": 164,
    "node_j": 166,
    "node_i_label": "Aston Zhang",
    "node_j_label": "Mu Li",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.2,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Po | x\u00bb) x Zz \u20ac topk(py(-|X))",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "# params BERT-Baseline (Lee et al., 2019)"
    ],
    "rationale": "Aston Zhang and Mu Li co-authored a book, suggesting a collaborative relationship."
  },
  {
    "node_i": 185,
    "node_j": 198,
    "node_i_label": "GPT",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ],
    "rationale": "The evidence suggests that GPT models can be used for policy learning through reinforcement learning techniques."
  },
  {
    "node_i": 185,
    "node_j": 197,
    "node_i_label": "GPT",
    "node_j_label": "Reinforcement Learning",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ],
    "rationale": "Evidence suggests GPT benefits from reinforcement learning techniques during training and post-training."
  },
  {
    "node_i": 194,
    "node_j": 197,
    "node_i_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "node_j_label": "Reinforcement Learning",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs"
    ],
    "rationale": "The evidence suggests that retrieval methods and distillation are related to reinforcement learning, particularly in the context of training and policy learning."
  },
  {
    "node_i": 195,
    "node_j": 197,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "Reinforcement Learning",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ],
    "rationale": "Evidence 1 directly links Multi-modal Models and Reinforcement Learning for NLP, suggesting a causal relationship where multi-modal models are used in reinforcement learning."
  },
  {
    "node_i": 195,
    "node_j": 196,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.8,
    "temporal": 0.7,
    "mechanistic": 0.5,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @"
    ],
    "rationale": "The evidence suggests that multi-modal models, including vision-language models, are a type of advanced model, indicating a causal relationship where the development of multi-modal models leads to the creation of vision-language models."
  },
  {
    "node_i": 196,
    "node_j": 200,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "RLHF",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ],
    "rationale": "Vision-Language Models are listed under Multi-modal LLMs, while RLHF is listed under Reinforcement Learning for NLP, suggesting a relationship within the broader field of NLP."
  },
  {
    "node_i": 196,
    "node_j": 198,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ],
    "rationale": "Vision-Language Models are listed under Multi-modal Models, which are related to Reinforcement Learning for NLP and Policy Learning."
  },
  {
    "node_i": 196,
    "node_j": 197,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "Reinforcement Learning",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ],
    "rationale": "Evidence suggests vision-language models are related to reinforcement learning, particularly in the context of NLP and policy learning, but the exact causal mechanism is not fully detailed."
  },
  {
    "node_i": 197,
    "node_j": 200,
    "node_i_label": "Reinforcement Learning",
    "node_j_label": "RLHF",
    "support": 0.8,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ],
    "rationale": "RLHF is presented as a subfield under Reinforcement Learning for NLP."
  },
  {
    "node_i": 175,
    "node_j": 178,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "2025",
    "support": 0.7992033720016479,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 163,
    "node_j": 179,
    "node_i_label": "MIT Press",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7969716906547546,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 159,
    "node_j": 175,
    "node_i_label": "Text Book",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7965311646461487,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 39,
    "node_j": 175,
    "node_i_label": "Deep Learning",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7960976302623749,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016."
    ]
  },
  {
    "node_i": 26,
    "node_j": 89,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "\u25aaRetriever",
    "support": 0.7959393918514251,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 173,
    "node_j": 175,
    "node_i_label": "Leandro von Werra",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7955900728702545,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 26,
    "node_j": 82,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "REALM \u25aa Pre-training Objective",
    "support": 0.7936756491661072,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 162,
    "node_j": 179,
    "node_i_label": "Aaron Courville",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7934474408626556,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 172,
    "node_j": 175,
    "node_i_label": "Lewis Tunstall",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7934123814105988,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 173,
    "node_j": 179,
    "node_i_label": "Leandro von Werra",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7933485507965088,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf."
    ]
  },
  {
    "node_i": 174,
    "node_j": 179,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7927880913019181,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 175,
    "node_j": 176,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Sebastian Raschka",
    "support": 0.7924245297908783,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 11,
    "node_j": 26,
    "node_i_label": "3",
    "node_j_label": "Retriever Training: REINFORCE (Policy Gradient",
    "support": 0.7915740132331848,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 161,
    "node_j": 179,
    "node_i_label": "Yoshua Bengio",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7905851066112518,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 167,
    "node_j": 175,
    "node_i_label": "Alexander J. Smola",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.788301819562912,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 3,
    "node_j": 175,
    "node_i_label": "IIIT BANGALORE",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7815559834241868,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "TULIKA SAHA, IIIT BANGALORE 28",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 171,
    "node_j": 179,
    "node_i_label": "World Scientific Publishing Co., Inc.",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.7789101541042328,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 168,
    "node_j": 175,
    "node_i_label": "Dive",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7789084017276764,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Natural Language Processing with Transformers.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 26,
    "node_j": 174,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "Thomas Wolf",
    "support": 0.7777450740337372,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 130,
    "node_j": 183,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Overview",
    "support": 0.7770897001028061,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 26,
    "node_j": 83,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "P(y|x",
    "support": 0.7765065789222717,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 26,
    "node_j": 28,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "P(y",
    "support": 0.7757582813501358,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 130,
    "node_j": 193,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Learning",
    "support": 0.7756351351737976,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 174,
    "node_j": 175,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7714716017246246,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 161,
    "node_j": 175,
    "node_i_label": "Yoshua Bengio",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7696829319000245,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 26,
    "node_j": 91,
    "node_i_label": "Retriever Training: REINFORCE (Policy Gradient",
    "node_j_label": "28",
    "support": 0.7640379518270493,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 130,
    "node_j": 196,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7633843809366226,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 130,
    "node_j": 192,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Strategies-II \u2022",
    "support": 0.7630441814661026,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 184,
    "node_j": 196,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7630114018917084,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 130,
    "node_j": 197,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7625804752111435,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 14,
    "node_j": 194,
    "node_i_label": "NLP",
    "node_j_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "support": 0.7602871894836426,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 130,
    "node_j": 198,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7596180737018585,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 14,
    "node_j": 77,
    "node_i_label": "NLP",
    "node_j_label": "Reader & Retriever \u25aaTrainable Components",
    "support": 0.7590355336666107,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)"
    ]
  },
  {
    "node_i": 175,
    "node_j": 181,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Wiley Publications",
    "support": 0.7588085919618607,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 33,
    "node_j": 130,
    "node_i_label": "Train Knowledge Time = (Be boys - on -",
    "node_j_label": "IIIT BANGALORE 7 Retrieval",
    "support": 0.7587016016244889,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 14,
    "node_j": 196,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7550015449523926,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Natural Language Processing with Transformers.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task"
    ]
  },
  {
    "node_i": 163,
    "node_j": 175,
    "node_i_label": "MIT Press",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7549911946058273,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "# params BERT-Baseline (Lee et al., 2019)"
    ]
  },
  {
    "node_i": 175,
    "node_j": 180,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Tanmoy Chakraborty",
    "support": 0.7535027623176574,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 130,
    "node_j": 187,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "\u2022 Multi",
    "support": 0.7529729008674622,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 188,
    "node_j": 196,
    "node_i_label": "\u2022 Fine-tuning",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7525039047002793,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 130,
    "node_j": 195,
    "node_i_label": "IIIT BANGALORE 7 Retrieval",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7515892952680587,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 183,
    "node_j": 196,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7511678040027618,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 143,
    "node_j": 159,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "Text Book",
    "support": 0.7496356517076492,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 175,
    "node_j": 177,
    "node_i_label": "\u25aa Build a Large Language Model",
    "node_j_label": "Manning Publications",
    "support": 0.7495678544044495,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g"
    ]
  },
  {
    "node_i": 14,
    "node_j": 191,
    "node_i_label": "NLP",
    "node_j_label": "Advanced Post-training",
    "support": 0.7468766897916794,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 14,
    "node_j": 80,
    "node_i_label": "NLP",
    "node_j_label": "Reader \u2013 LM \u25aaPre",
    "support": 0.7466058731079102,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "= log P(y \u2223 x, z) \u25aaFor example, o If Doc A retrieved \u2192 reader predicts [MASK] with 95% confidence --> R(A) = log(0.95)",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 193,
    "node_j": 196,
    "node_i_label": "Learning",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7462854951620101,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 187,
    "node_j": 196,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7452965795993804,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task"
    ]
  },
  {
    "node_i": 78,
    "node_j": 80,
    "node_i_label": "Document Encoder",
    "node_j_label": "Reader \u2013 LM \u25aaPre",
    "support": 0.7439376294612885,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Input LM ey >| | Output",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 77,
    "node_j": 80,
    "node_i_label": "Reader & Retriever \u25aaTrainable Components",
    "node_j_label": "Reader \u2013 LM \u25aaPre",
    "support": 0.7377566635608673,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 194,
    "node_j": 199,
    "node_i_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "node_j_label": "the Loop Feedback",
    "support": 0.7373454660177231,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 34,
    "node_j": 196,
    "node_i_label": "Model Docs",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7365206182003021,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "= log P(y \u2223 x, z) \u25aaFor example, o If Doc A retrieved \u2192 reader predicts [MASK] with 95% confidence --> R(A) = log(0.95)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 193,
    "node_i_label": "NLP",
    "node_j_label": "Learning",
    "support": 0.7362256199121475,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 191,
    "node_j": 196,
    "node_i_label": "Advanced Post-training",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7357051730155945,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 33,
    "node_j": 196,
    "node_i_label": "Train Knowledge Time = (Be boys - on -",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7341985315084457,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 143,
    "node_j": 149,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "P. Bhattacharyya",
    "support": 0.7337179243564605,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "= log P(y \u2223 x, z) TULIKA SAHA, IIIT BANGALORE 27",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o",
      "TULIKA SAHA, IIIT BANGALORE 28",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 192,
    "node_j": 196,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7316198259592056,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 39,
    "node_j": 168,
    "node_i_label": "Deep Learning",
    "node_j_label": "Dive",
    "support": 0.7314030021429062,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 78,
    "node_j": 205,
    "node_i_label": "Document Encoder",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.7303448796272278,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 195,
    "node_i_label": "NLP",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7290746450424195,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 14,
    "node_j": 34,
    "node_i_label": "NLP",
    "node_j_label": "Model Docs",
    "support": 0.7283089756965637,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 14,
    "node_j": 188,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Fine-tuning",
    "support": 0.7277314841747284,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 80,
    "node_j": 144,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "Stanford",
    "support": 0.7276871502399445,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 171,
    "node_j": 175,
    "node_i_label": "World Scientific Publishing Co., Inc.",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.7275865554809571,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 80,
    "node_j": 207,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "Computational Cost",
    "support": 0.7272527307271958,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 14,
    "node_j": 183,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Overview",
    "support": 0.7266071617603302,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ]
  },
  {
    "node_i": 34,
    "node_j": 191,
    "node_i_label": "Model Docs",
    "node_j_label": "Advanced Post-training",
    "support": 0.7259056806564331,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ]
  },
  {
    "node_i": 82,
    "node_j": 85,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "Doc",
    "support": 0.7257860004901886,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 14,
    "node_j": 198,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.724541375041008,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Natural Language Processing with Transformers.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 197,
    "node_i_label": "NLP",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7236123949289321,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)"
    ]
  },
  {
    "node_i": 39,
    "node_j": 177,
    "node_i_label": "Deep Learning",
    "node_j_label": "Manning Publications",
    "support": 0.7232302099466323,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 143,
    "node_j": 144,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "Stanford",
    "support": 0.7216133296489715,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Columbia University.|University.",
      "Columbia University.|University."
    ]
  },
  {
    "node_i": 34,
    "node_j": 193,
    "node_i_label": "Model Docs",
    "node_j_label": "Learning",
    "support": 0.7206528156995773,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 198,
    "node_j": 199,
    "node_i_label": "\u2022 Policy Learning",
    "node_j_label": "the Loop Feedback",
    "support": 0.7205691039562225,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 193,
    "node_j": 195,
    "node_i_label": "Learning",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7188612341880798,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 80,
    "node_j": 143,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "IIIT BANGALORE 1 Slides",
    "support": 0.7184721499681472,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 13 \u25aaLimitations:"
    ]
  },
  {
    "node_i": 187,
    "node_j": 198,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7178585410118103,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 183,
    "node_j": 198,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7152105331420898,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 78,
    "node_j": 206,
    "node_i_label": "Document Encoder",
    "node_j_label": "Memory Requirement",
    "support": 0.7143559098243714,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 79,
    "node_j": 205,
    "node_i_label": "Query Encoder",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.7137590050697327,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Indexed Contexts (keys) ...",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 39,
    "node_j": 143,
    "node_i_label": "Deep Learning",
    "node_j_label": "IIIT BANGALORE 1 Slides",
    "support": 0.7126858353614807,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 78,
    "node_j": 208,
    "node_i_label": "Document Encoder",
    "node_j_label": "\u25aaWould",
    "support": 0.7114558428525924,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 192,
    "node_i_label": "NLP",
    "node_j_label": "Strategies-II \u2022",
    "support": 0.709892001748085,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Natural Language Processing with Transformers.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 193,
    "node_j": 199,
    "node_i_label": "Learning",
    "node_j_label": "the Loop Feedback",
    "support": 0.7092069864273072,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 39,
    "node_j": 181,
    "node_i_label": "Deep Learning",
    "node_j_label": "Wiley Publications",
    "support": 0.7091768175363541,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 14,
    "node_j": 187,
    "node_i_label": "NLP",
    "node_j_label": "\u2022 Multi",
    "support": 0.7089264839887619,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 187,
    "node_j": 197,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7088432520627975,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 79,
    "node_j": 206,
    "node_i_label": "Query Encoder",
    "node_j_label": "Memory Requirement",
    "support": 0.7083703368902207,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 82,
    "node_j": 204,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "13",
    "support": 0.7078821897506714,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 187,
    "node_j": 193,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "Learning",
    "support": 0.7070366352796554,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 14,
    "node_j": 78,
    "node_i_label": "NLP",
    "node_j_label": "Document Encoder",
    "support": 0.7068281680345535,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 189,
    "node_i_label": "NLP",
    "node_j_label": "Efficient Fine",
    "support": 0.7068032264709473,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Natural Language Processing with Transformers."
    ]
  },
  {
    "node_i": 183,
    "node_j": 193,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Learning",
    "support": 0.7060031712055206,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 78,
    "node_j": 207,
    "node_i_label": "Document Encoder",
    "node_j_label": "Computational Cost",
    "support": 0.7046526998281479,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 169,
    "node_j": 207,
    "node_i_label": "\u25aa Daniel Graupe",
    "node_j_label": "Computational Cost",
    "support": 0.7044069349765778,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "o Memory Requirement o Computational Cost o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 191,
    "node_j": 195,
    "node_i_label": "Advanced Post-training",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7037585437297821,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ]
  },
  {
    "node_i": 34,
    "node_j": 197,
    "node_i_label": "Model Docs",
    "node_j_label": "Reinforcement Learning",
    "support": 0.7032277226448059,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 197,
    "node_j": 198,
    "node_i_label": "Reinforcement Learning",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7028891324996949,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 193,
    "node_j": 198,
    "node_i_label": "Learning",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7016630530357361,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 14,
    "node_j": 190,
    "node_i_label": "NLP",
    "node_j_label": "Mandate 3:",
    "support": 0.7013584613800049,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Natural Language Processing with Transformers.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 82,
    "node_j": 87,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "70%",
    "support": 0.7009221285581588,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 104,
    "node_j": 109,
    "node_i_label": "TS",
    "node_j_label": "Chen et al.",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Transformer Seq2Seq TS (Multitask) 34.5 37.4 - 11318m DrQA (Chen et al., 2017)",
      "# params BERT-Baseline (Lee et al., 2019)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf."
    ],
    "rationale": "The evidence mentions TS and Chen et al. in the same context, suggesting a relationship, but the nature of the relationship isn't explicitly causal."
  },
  {
    "node_i": 99,
    "node_j": 119,
    "node_i_label": "110m TS",
    "node_j_label": "ORQA",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Transformer Seq2Seq TS (Multitask) 27.0 29.1 - 223m TS (large) (Roberts et al., 2020)",
      "16.8 fever fevvneereeoneeeeeeee? 3.9 Poe C8 2 a ROO coe Py \u00bb a N a aa 1 2 8 64 256 1024 k (# nearest neighbors)"
    ],
    "rationale": "ORQA uses 110m PathRetriever, suggesting a potential causal link where 110m TS influences ORQA's architecture or performance."
  },
  {
    "node_i": 172,
    "node_j": 177,
    "node_i_label": "Lewis Tunstall",
    "node_j_label": "Manning Publications",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Indexed Keys (N Values (N,F Obama Obama _was was born born in|Obama in\\Obama was was born born in in Hawaii] Hawaii and|'and graduated from Columbia] University] land graduated fromland graduated from Columbia.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ],
    "rationale": "Lewis Tunstall is listed as an author, and Manning Publications is listed as the publisher of a book he co-authored, suggesting a professional relationship."
  },
  {
    "node_i": 185,
    "node_j": 200,
    "node_i_label": "GPT",
    "node_j_label": "RLHF",
    "support": 0.7,
    "temporal": 0.8,
    "mechanistic": 0.5,
    "contexts": [
      "Dense Retr.+-Transformer REALM 40.4 40.7 42.9 330m",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Datastore 8 \u00a3 ro \u00a3 g fF - 36 Ayeaydiag Bj 17 u +34 - 32 Axaidag uyewop-uy 16 a a - 30 voneydepy 15 a - 28 ujewog f fF - 26 14 & 2 \u00b0 0.2 0.4 0.6 0.8 A (interpolation parameter)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "DR."
    ],
    "rationale": "RLHF is used to train GPT models, suggesting a temporal and mechanistic link."
  },
  {
    "node_i": 185,
    "node_j": 196,
    "node_i_label": "GPT",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.3,
    "contexts": [
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ],
    "rationale": "GPT is mentioned as a representative pre-trained model, suggesting it influences the development of other models like vision-language models."
  },
  {
    "node_i": 185,
    "node_j": 195,
    "node_i_label": "GPT",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Indexed Contexts (keys) ...",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ],
    "rationale": "The evidence suggests that GPT is related to multi-modal models as they are both mentioned in the context of advanced models and LLMs, but the exact causal mechanism is not explicitly described."
  },
  {
    "node_i": 185,
    "node_j": 194,
    "node_i_label": "GPT",
    "node_j_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ],
    "rationale": "GPT is a pre-trained model, and training methods like Retrieval & RAG and Distillation are used to further develop and refine these models."
  },
  {
    "node_i": 185,
    "node_j": 192,
    "node_i_label": "GPT",
    "node_j_label": "Strategies-II \u2022",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.3,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ],
    "rationale": "The evidence mentions GPT in the context of pre-trained models and advanced post-training strategies, suggesting a relationship to strategies."
  },
  {
    "node_i": 185,
    "node_j": 191,
    "node_i_label": "GPT",
    "node_j_label": "Advanced Post-training",
    "support": 0.7,
    "temporal": 0.8,
    "mechanistic": 0.5,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ],
    "rationale": "The evidence suggests that GPT is a pre-trained model, and advanced post-training strategies are applied to it, indicating a causal relationship where GPT's architecture enables subsequent post-training."
  },
  {
    "node_i": 185,
    "node_j": 189,
    "node_i_label": "GPT",
    "node_j_label": "Efficient Fine",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Dense Retr.+-Transformer REALM 40.4 40.7 42.9 330m",
      "o Memory Requirement o Computational Cost o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ],
    "rationale": "GPT is mentioned in the context of pre-trained models and fine-tuning, suggesting a potential influence on efficient fine-tuning methods."
  },
  {
    "node_i": 194,
    "node_j": 196,
    "node_i_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "node_j_label": "\u2022 Vision-Language Models",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ],
    "rationale": "The evidence mentions retrieval methods, distillation, and vision-language models in the same context, suggesting a relationship, but the exact causal mechanism isn't clearly specified."
  },
  {
    "node_i": 194,
    "node_j": 195,
    "node_i_label": "Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:"
    ],
    "rationale": "Training methods like RAG involve retrieval and distillation which can influence the development of multi-modal models."
  },
  {
    "node_i": 195,
    "node_j": 200,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "RLHF",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.3,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Indexed Contexts (keys) ..."
    ],
    "rationale": "Multi-modal models are listed alongside RLHF in the document, suggesting a relationship, but the exact nature is not explicitly stated."
  },
  {
    "node_i": 195,
    "node_j": 198,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "= -log P(y|x,z) \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ],
    "rationale": "Multi-modal models are mentioned alongside policy learning in the context of reinforcement learning, suggesting a potential relationship."
  },
  {
    "node_i": 196,
    "node_j": 199,
    "node_i_label": "\u2022 Vision-Language Models",
    "node_j_label": "the Loop Feedback",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task"
    ],
    "rationale": "Vision-Language Models are related to Reinforcement Learning with Human in the Loop Feedback, suggesting a potential influence."
  },
  {
    "node_i": 197,
    "node_j": 201,
    "node_i_label": "Reinforcement Learning",
    "node_j_label": "4",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "+ APinn O12)"
    ],
    "rationale": "Evidence 1 mentions 'Reinforcement Learning for NLP' and 'TULIKA SAHA, IIIT BANGALORE 4', suggesting a connection, although the nature of the relationship is not explicitly causal."
  },
  {
    "node_i": 85,
    "node_j": 90,
    "node_i_label": "Doc",
    "node_j_label": "Doc A. o",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "DR.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)"
    ],
    "rationale": "Evidence suggests Doc A influences retriever behavior, but the direct causal link to Doc is weak."
  },
  {
    "node_i": 85,
    "node_j": 87,
    "node_i_label": "Doc",
    "node_j_label": "70%",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "DR.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ],
    "rationale": "The evidence suggests a correlation between Doc B retrieval and a 70% confidence score, but lacks strong temporal or mechanistic links to establish causality."
  },
  {
    "node_i": 86,
    "node_j": 90,
    "node_i_label": "95%",
    "node_j_label": "Doc A. o",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "DR.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d"
    ],
    "rationale": "Doc A yields high reward, causing retriever embeddings to adjust, suggesting a causal influence."
  },
  {
    "node_i": 86,
    "node_j": 87,
    "node_i_label": "95%",
    "node_j_label": "70%",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Hawaii Illinois |0.2 |0.2 Parametric distribution",
      "= log P(y \u2223 x, z) \u25aaFor example, o If Doc A retrieved \u2192 reader predicts [MASK] with 95% confidence --> R(A) = log(0.95)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @"
    ],
    "rationale": "The evidence mentions probabilities and confidence levels, suggesting a potential influence, but lacks direct causal links."
  },
  {
    "node_i": 87,
    "node_j": 90,
    "node_i_label": "70%",
    "node_j_label": "Doc A. o",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "DR.",
      "16.8 fever fevvneereeoneeeeeeee? 3.9 Poe C8 2 a ROO coe Py \u00bb a N a aa 1 2 8 64 256 1024 k (# nearest neighbors)"
    ],
    "rationale": "The retriever learns to consistently fetch the most helpful documents (Doc A. o) because it yielded high reward, suggesting a causal link to 70% confidence."
  },
  {
    "node_i": 87,
    "node_j": 89,
    "node_i_label": "70%",
    "node_j_label": "\u25aaRetriever",
    "support": 0.7,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Aggregation = = Se S\u2014 lyenpth)",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "16.8 fever fevvneereeoneeeeeeee? 3.9 Poe C8 2 a ROO coe Py \u00bb a N a aa 1 2 8 64 256 1024 k (# nearest neighbors)"
    ],
    "rationale": "The retriever uses a gradient signal to increase the probability of picking a document with 70% confidence."
  },
  {
    "node_i": 37,
    "node_j": 140,
    "node_i_label": "borntin",
    "node_j_label": "MD",
    "support": 0.7,
    "temporal": 0.6,
    "mechanistic": 0.3,
    "contexts": [
      "DR.",
      "Barack is Married to Michelle and their first daughter, Barack is married to | Michelle Oddo Obama was born in| Hawaii Obama is a native of | Hawaii Classification PLM(y) Test Context Target Representation Obama's MD birthplace is a q=f q= f(z) )",
      "Pa 6",
      "Obama was senator for Illinois from 1997 to 2005, Obama was borntin ....",
      "Indexed Keys (N Values (N,F Obama Obama _was was born born in|Obama in\\Obama was was born born in in Hawaii] Hawaii and|'and graduated from Columbia] University] land graduated fromland graduated from Columbia."
    ],
    "rationale": "Evidence suggests 'borntin' might be related to Obama's origins, potentially influencing his later life or career (MD)."
  },
  {
    "node_i": 39,
    "node_j": 166,
    "node_i_label": "Deep Learning",
    "node_j_label": "Mu Li",
    "support": 0.6988522440195084,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 82,
    "node_j": 88,
    "node_i_label": "REALM \u25aa Pre-training Objective",
    "node_j_label": "5%",
    "support": 0.6983321249485016,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ]
  },
  {
    "node_i": 193,
    "node_j": 197,
    "node_i_label": "Learning",
    "node_j_label": "Reinforcement Learning",
    "support": 0.6981014043092728,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 184,
    "node_j": 195,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.698030436038971,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 39,
    "node_j": 171,
    "node_i_label": "Deep Learning",
    "node_j_label": "World Scientific Publishing Co., Inc.",
    "support": 0.6962847441434861,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 14,
    "node_j": 201,
    "node_i_label": "NLP",
    "node_j_label": "4",
    "support": 0.6959917634725571,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Natural Language Processing with Transformers.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020"
    ]
  },
  {
    "node_i": 184,
    "node_j": 197,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "Reinforcement Learning",
    "support": 0.695552545785904,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 188,
    "node_j": 195,
    "node_i_label": "\u2022 Fine-tuning",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6950279891490936,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 13 \u25aaLimitations:"
    ]
  },
  {
    "node_i": 193,
    "node_j": 200,
    "node_i_label": "Learning",
    "node_j_label": "RLHF",
    "support": 0.6946500182151795,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 143,
    "node_j": 146,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "UMass",
    "support": 0.6932398855686188,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "Columbia University.|University."
    ]
  },
  {
    "node_i": 192,
    "node_j": 197,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "Reinforcement Learning",
    "support": 0.692205473780632,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 80,
    "node_j": 146,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "UMass",
    "support": 0.6917864620685578,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26"
    ]
  },
  {
    "node_i": 143,
    "node_j": 160,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "Ian Goodfellow",
    "support": 0.6915785372257233,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs"
    ]
  },
  {
    "node_i": 186,
    "node_j": 191,
    "node_i_label": "Mandate 2:",
    "node_j_label": "Advanced Post-training",
    "support": 0.6913096725940704,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 78,
    "node_j": 146,
    "node_i_label": "Document Encoder",
    "node_j_label": "UMass",
    "support": 0.6912830084562301,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 79,
    "node_j": 207,
    "node_i_label": "Query Encoder",
    "node_j_label": "Computational Cost",
    "support": 0.6910345554351807,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ]
  },
  {
    "node_i": 206,
    "node_j": 207,
    "node_i_label": "Memory Requirement",
    "node_j_label": "Computational Cost",
    "support": 0.6910264104604721,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 79,
    "node_i_label": "NLP",
    "node_j_label": "Query Encoder",
    "support": 0.6908588349819184,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Natural Language Processing with Transformers.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 14,
    "node_j": 81,
    "node_i_label": "NLP",
    "node_j_label": "26",
    "support": 0.6905196726322174,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "# params BERT-Baseline (Lee et al., 2019)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 184,
    "node_j": 198,
    "node_i_label": "\u2022 Representative",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.6904240101575851,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:"
    ]
  },
  {
    "node_i": 190,
    "node_j": 191,
    "node_i_label": "Mandate 3:",
    "node_j_label": "Advanced Post-training",
    "support": 0.6900470286607743,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 205,
    "node_j": 207,
    "node_i_label": "\u25aaLimitations:",
    "node_j_label": "Computational Cost",
    "support": 0.6899703353643417,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 159,
    "node_j": 166,
    "node_i_label": "Text Book",
    "node_j_label": "Mu Li",
    "support": 0.6885635465383529,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "How to use the Book? /",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 14,
    "node_j": 185,
    "node_i_label": "NLP",
    "node_j_label": "GPT",
    "support": 0.6873183190822602,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Natural Language Processing with Transformers.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "= log P(y \u2223 x, z) \u25aaFor example, o If Doc A retrieved \u2192 reader predicts [MASK] with 95% confidence --> R(A) = log(0.95)"
    ]
  },
  {
    "node_i": 189,
    "node_j": 195,
    "node_i_label": "Efficient Fine",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6863542199134827,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3"
    ]
  },
  {
    "node_i": 190,
    "node_j": 198,
    "node_i_label": "Mandate 3:",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.685569766163826,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 80,
    "node_j": 145,
    "node_i_label": "Reader \u2013 LM \u25aaPre",
    "node_j_label": "C. Manning",
    "support": 0.6854312479496002,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "# params BERT-Baseline (Lee et al., 2019)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 146,
    "node_j": 159,
    "node_i_label": "UMass",
    "node_j_label": "Text Book",
    "support": 0.6840496957302094,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "How to use the Book? /",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 192,
    "node_j": 193,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "Learning",
    "support": 0.683394393324852,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "How to use the Book? /"
    ]
  },
  {
    "node_i": 187,
    "node_j": 195,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6833462625741958,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d"
    ]
  },
  {
    "node_i": 171,
    "node_j": 206,
    "node_i_label": "World Scientific Publishing Co., Inc.",
    "node_j_label": "Memory Requirement",
    "support": 0.682925745844841,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 14,
    "node_j": 41,
    "node_i_label": "NLP",
    "node_j_label": "Quantization",
    "support": 0.6828564673662185,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Natural Language Processing with Transformers.",
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 14,
    "node_j": 199,
    "node_i_label": "NLP",
    "node_j_label": "the Loop Feedback",
    "support": 0.6811994045972825,
    "temporal": 0.0,
    "mechanistic": 0.8,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 34,
    "node_j": 183,
    "node_i_label": "Model Docs",
    "node_j_label": "\u2022 Overview",
    "support": 0.6807682275772095,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Indexed Contexts (keys) ..."
    ]
  },
  {
    "node_i": 183,
    "node_j": 195,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6807333558797837,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020."
    ]
  },
  {
    "node_i": 41,
    "node_j": 193,
    "node_i_label": "Quantization",
    "node_j_label": "Learning",
    "support": 0.6800093799829483,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 186,
    "node_j": 198,
    "node_i_label": "Mandate 2:",
    "node_j_label": "\u2022 Policy Learning",
    "support": 0.6799672052264214,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 88,
    "node_j": 90,
    "node_i_label": "5%",
    "node_j_label": "Doc A. o",
    "support": 0.679862454533577,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "DR."
    ]
  },
  {
    "node_i": 207,
    "node_j": 208,
    "node_i_label": "Computational Cost",
    "node_j_label": "\u25aaWould",
    "support": 0.6786350280046463,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 197,
    "node_j": 199,
    "node_i_label": "Reinforcement Learning",
    "node_j_label": "the Loop Feedback",
    "support": 0.6779278129339218,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "RAG: REALM \u25aa Retriever Training: REINFORCE (Policy Gradient)"
    ]
  },
  {
    "node_i": 191,
    "node_j": 199,
    "node_i_label": "Advanced Post-training",
    "node_j_label": "the Loop Feedback",
    "support": 0.6745476275682449,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016."
    ]
  },
  {
    "node_i": 78,
    "node_j": 142,
    "node_i_label": "Document Encoder",
    "node_j_label": "102",
    "support": 0.6740961492061615,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 34,
    "node_j": 189,
    "node_i_label": "Model Docs",
    "node_j_label": "Efficient Fine",
    "support": 0.6736224710941314,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Algorithm o Retriever must be updated based on whether the documents it picked helped the reader predict correctly o REALM treats retrieval as a policy over documents o Reward = log-likelihood of correct token given retrieved doc \u25aaR(z)",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)"
    ]
  },
  {
    "node_i": 79,
    "node_j": 208,
    "node_i_label": "Query Encoder",
    "node_j_label": "\u25aaWould",
    "support": 0.6731336086988449,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)"
    ]
  },
  {
    "node_i": 143,
    "node_j": 165,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "Zachary C. Lipton",
    "support": 0.6714861512184143,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)"
    ]
  },
  {
    "node_i": 145,
    "node_j": 159,
    "node_i_label": "C. Manning",
    "node_j_label": "Text Book",
    "support": 0.6710483640432358,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "How to use the Book? /",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:"
    ]
  },
  {
    "node_i": 143,
    "node_j": 145,
    "node_i_label": "IIIT BANGALORE 1 Slides",
    "node_j_label": "C. Manning",
    "support": 0.6710103958845138,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "TULIKA SAHA, IIIT BANGALORE 28",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 34,
    "node_j": 184,
    "node_i_label": "Model Docs",
    "node_j_label": "\u2022 Representative",
    "support": 0.6693486243486404,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "# params BERT-Baseline (Lee et al., 2019)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 192,
    "node_j": 195,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6689423143863678,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @"
    ]
  },
  {
    "node_i": 132,
    "node_j": 153,
    "node_i_label": "Modify the LM",
    "node_j_label": "Exam & Quiz",
    "support": 0.6684932023286819,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Input LM ey >| | Output",
      "For e.g., kNN-LMs \u25aa Intermediate Fusion o Modify the LM architecture to be aware of the book o For e.g., ToolFormer, FLARE, RETRO etc. \u25aa Input augmentation (RAG)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 47,
    "node_j": 153,
    "node_i_label": "Apaun(y)+(1",
    "node_j_label": "Exam & Quiz",
    "support": 0.6680339843034744,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "=Apaun(y)+(1\u2014A}pu pty) ty) Test Context Target Representation x q= q= _ f(z)",
      "+ APinn O12)",
      "= (1- A)Pim =)",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 135,
    "node_j": 153,
    "node_i_label": "RAG",
    "node_j_label": "Exam & Quiz",
    "support": 0.6664598152041435,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "o Before you start solving o For e.g., RAG, REALM etc.",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "How to use the Book? /",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 187,
    "node_j": 199,
    "node_i_label": "\u2022 Multi",
    "node_j_label": "the Loop Feedback",
    "support": 0.6657453447580337,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?",
      "Transformer Seq2Seq TS (Multitask) 29.8 32.2 - 738m TS (11b) (Roberts et al., 2020)"
    ]
  },
  {
    "node_i": 192,
    "node_j": 199,
    "node_i_label": "Strategies-II \u2022",
    "node_j_label": "the Loop Feedback",
    "support": 0.6650861591100693,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @"
    ]
  },
  {
    "node_i": 79,
    "node_j": 146,
    "node_i_label": "Query Encoder",
    "node_j_label": "UMass",
    "support": 0.6632546067237854,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)"
    ]
  },
  {
    "node_i": 174,
    "node_j": 181,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "Wiley Publications",
    "support": 0.6630848303437233,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to use the Book? /",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g"
    ]
  },
  {
    "node_i": 79,
    "node_j": 142,
    "node_i_label": "Query Encoder",
    "node_j_label": "102",
    "support": 0.6627200931310654,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Transformers 6 TULIKA SAHA, IIIT-BANGALORE Decoder Encoder Attention is all you need (Vaswani et al., 2017)",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 188,
    "node_j": 189,
    "node_i_label": "\u2022 Fine-tuning",
    "node_j_label": "Efficient Fine",
    "support": 0.6611572533845902,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "o Memory Requirement o Computational Cost o",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 190,
    "node_j": 195,
    "node_i_label": "Mandate 3:",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6608657702803612,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Indexed Contexts (keys) ...",
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:"
    ]
  },
  {
    "node_i": 105,
    "node_j": 122,
    "node_i_label": "11b",
    "node_j_label": "Wikipedia",
    "support": 0.6603322178125381,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "+ APinn O12)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 183,
    "node_j": 199,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "the Loop Feedback",
    "support": 0.6600900888442993,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ]
  },
  {
    "node_i": 48,
    "node_j": 153,
    "node_i_label": "q=",
    "node_j_label": "Exam & Quiz",
    "support": 0.6600436389446258,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "=encg(z), q(x) = = enc,(z)",
      "Bake at in Train Knowledge Time = (Be boys - on - \u201cClosed book\u201d",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4"
    ]
  },
  {
    "node_i": 35,
    "node_j": 79,
    "node_i_label": "Open",
    "node_j_label": "Query Encoder",
    "support": 0.6594581693410874,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aaJoint Pre-training of Reader & Retriever \u25aaTrainable Components o Retriever - Document Encoder, Query Encoder o Reader \u2013 LM \u25aaPre-training Objective: TULIKA SAHA, IIIT BANGALORE 26",
      "\u00a9 Input Output Embedding Embedding Outputs (shifted right)",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 34,
    "node_j": 199,
    "node_i_label": "Model Docs",
    "node_j_label": "the Loop Feedback",
    "support": 0.6579575091600418,
    "temporal": 0.0,
    "mechanistic": 0.8,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 4,
    "node_j": 122,
    "node_i_label": "al.",
    "node_j_label": "Wikipedia",
    "support": 0.6570942223072052,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Ilinois | 0.2 Obama's birthplace is OOOO)"
    ]
  },
  {
    "node_i": 34,
    "node_j": 41,
    "node_i_label": "Model Docs",
    "node_j_label": "Quantization",
    "support": 0.6564198106527328,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Nonparametric distribution Training Ci Contexts Targets Ui || Representations ki = f(cj) Distances dj = d(q, kj) Nearest k Normalization p(ki) \u00ab",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 183,
    "node_j": 186,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Mandate 2:",
    "support": 0.6564009040594101,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 186,
    "node_j": 195,
    "node_i_label": "Mandate 2:",
    "node_j_label": "Models \u2022 Multi-modal",
    "support": 0.6537778869271278,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Indexed Contexts (keys) ..."
    ]
  },
  {
    "node_i": 209,
    "node_j": 216,
    "node_i_label": "trillions",
    "node_j_label": "\u2022 How",
    "support": 0.6529964163899422,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Aggregation = = Se S\u2014 lyenpth)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)"
    ]
  },
  {
    "node_i": 183,
    "node_j": 190,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Mandate 3:",
    "support": 0.6526997208595275,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 176,
    "node_j": 181,
    "node_i_label": "Sebastian Raschka",
    "node_j_label": "Wiley Publications",
    "support": 0.6516284048557281,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "How to use the Book? /",
      "Pre-trained LLMs - Closed Book Vs Open Book Exams TULIKA SAHA, IIIT BANGALORE 7 Retrieval based LLMs Pre-trained LLMs",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o"
    ]
  },
  {
    "node_i": 117,
    "node_j": 122,
    "node_i_label": "Asai",
    "node_j_label": "Wikipedia",
    "support": 0.6478118419647216,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "+ APinn O12)",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "Ilinois | 0.2 Obama's birthplace is OOOO)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 146,
    "node_j": 150,
    "node_i_label": "UMass",
    "node_j_label": "IIT-Delhi",
    "support": 0.6471543639898301,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Columbia University.|University.",
      "Cj Uy ki = F(ci) ... ..",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "TULIKA SAHA, IIIT BANGALORE 28",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 18,
    "node_j": 54,
    "node_i_label": "15",
    "node_j_label": "billions",
    "support": 0.6460234612226486,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "16.8 fever fevvneereeoneeeeeeee? 3.9 Poe C8 2 a ROO coe Py \u00bb a N a aa 1 2 8 64 256 1024 k (# nearest neighbors)"
    ]
  },
  {
    "node_i": 183,
    "node_j": 192,
    "node_i_label": "\u2022 Overview",
    "node_j_label": "Strategies-II \u2022",
    "support": 0.6399892166256904,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "How to use the Book? /",
      "How to fine-tune on downstream task? \u25aaWould need the most similar \u201cinput\u201d \u25aai.e., need examples labeled for the target task \u25aaNot clear how to organize unstructured text as input-output pairs for the desired task",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 166,
    "node_j": 207,
    "node_i_label": "Mu Li",
    "node_j_label": "Computational Cost",
    "support": 0.638711829483509,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "o Memory Requirement o Computational Cost o",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 9 Credit: GENERALIZATION THROUGH MEMORIZATION:"
    ]
  },
  {
    "node_i": 174,
    "node_j": 177,
    "node_i_label": "Thomas Wolf",
    "node_j_label": "Manning Publications",
    "support": 0.6368502348661422,
    "temporal": 0.0,
    "mechanistic": 0.6,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "How to use the Book? /",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 59,
    "node_j": 63,
    "node_i_label": "UB",
    "node_j_label": "ROO",
    "support": 0.6354605823755264,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "o Before you start solving o For e.g., RAG, REALM etc.",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)"
    ]
  },
  {
    "node_i": 166,
    "node_j": 171,
    "node_i_label": "Mu Li",
    "node_j_label": "World Scientific Publishing Co., Inc.",
    "support": 0.6353148072957993,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi"
    ]
  },
  {
    "node_i": 65,
    "node_j": 66,
    "node_i_label": "1 2",
    "node_j_label": "256 1024",
    "support": 0.6352824911475181,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "o Memory Requirement o Computational Cost o",
      "= (1- A)Pim =)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "This causes two issues: o Restricts the size of corpus that can be indexed o k-neighbors returns only k tokens \u25aa What if we retrieve the entire continuation instead of just one token?"
    ]
  },
  {
    "node_i": 50,
    "node_j": 54,
    "node_i_label": "5",
    "node_j_label": "billions",
    "support": 0.6315828323364258,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Pa 6",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025."
    ]
  },
  {
    "node_i": 116,
    "node_j": 122,
    "node_i_label": "PathRetriever",
    "node_j_label": "Wikipedia",
    "support": 0.6286001920700073,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)"
    ]
  },
  {
    "node_i": 163,
    "node_j": 181,
    "node_i_label": "MIT Press",
    "node_j_label": "Wiley Publications",
    "support": 0.6273838594555855,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Columbia University.|University.",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "External Model Docs can use at Test \u201coN \u201cye oy es\" see \u201cOpen book\u201d",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 20,
    "node_j": 122,
    "node_i_label": "Roberts",
    "node_j_label": "Wikipedia",
    "support": 0.6211097702383995,
    "temporal": 0.2,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Obama was born in Hawaii, and graduated from Obama| was Columbia University. ...",
      "TULIKA SAHA EMAIL: TULIKA.SAHA@IIITB.AC.IN OFFICE: 102-E (ARYABHATA) TULIKA SAHA, IIIT BANGALORE 1 Slides are adopted from the Stanford course 'NLP with DL\u2019 by C. Manning, UMass course \u2018Advanced NLP\u2019 by M Iyyer, 'NLP with DL' by P. Bhattacharyya and LCS2 at IIT-Delhi",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "When to retrieve? TULIKA SAHA, IIIT BANGALORE 8 \u25aa Need to search the question/query in the book - Retrieval \u25aaOutput Interpolations o After solving the question yourself o"
    ]
  },
  {
    "node_i": 177,
    "node_j": 180,
    "node_i_label": "Manning Publications",
    "node_j_label": "Tanmoy Chakraborty",
    "support": 0.6138563394546509,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "How to use the Book? /",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 21,
    "node_j": 210,
    "node_i_label": "9",
    "node_j_label": "2022",
    "support": 0.6137885674834251,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Hawaii |0.2",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ]
  },
  {
    "node_i": 166,
    "node_j": 205,
    "node_i_label": "Mu Li",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.6122331231832504,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "For e.g., kNN-LMs \u25aa Intermediate Fusion o Modify the LM architecture to be aware of the book o For e.g., ToolFormer, FLARE, RETRO etc. \u25aa Input augmentation (RAG)"
    ]
  },
  {
    "node_i": 166,
    "node_j": 177,
    "node_i_label": "Mu Li",
    "node_j_label": "Manning Publications",
    "support": 0.6098399981856346,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "# params BERT-Baseline (Lee et al., 2019)",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents.",
      "Columbia University.|University."
    ]
  },
  {
    "node_i": 81,
    "node_j": 205,
    "node_i_label": "26",
    "node_j_label": "\u25aaLimitations:",
    "support": 0.6095624923706054,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Pa 6",
      "Sparse Retr.+DocReader N/A - 20.7 25.7 34m HardEM (Min et al., 2019a)",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk.",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ]
  },
  {
    "node_i": 161,
    "node_j": 163,
    "node_i_label": "Yoshua Bengio",
    "node_j_label": "MIT Press",
    "support": 0.6082770317792893,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "Text Book \u25aa Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "o If Doc B retrieved \u2192 [MASK] with 70% confidence --> R(B) = log(0.70)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Two advantages: \u2022 For same corpus, # of indexed keys reduce by a fraction of |N| = size of each chunk."
    ]
  },
  {
    "node_i": 168,
    "node_j": 177,
    "node_i_label": "Dive",
    "node_j_label": "Manning Publications",
    "support": 0.6042516484856606,
    "temporal": 0.0,
    "mechanistic": 0.4,
    "contexts": [
      "RAG: REALM \u25aa Pre-training Objective: \u25aa Reader Training: Given a specific document z, the reader predicts the masked token y[mask] o This is trained via normal cross-entropy loss: L(reader)",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Indexed Keys (N Values (N,F Obama Obama _was was born born in|Obama in\\Obama was was born born in in Hawaii] Hawaii and|'and graduated from Columbia] University] land graduated fromland graduated from Columbia.",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ]
  },
  {
    "node_i": 19,
    "node_j": 125,
    "node_i_label": "2019",
    "node_j_label": "46.8 330",
    "support": 0.6,
    "temporal": 0.7,
    "mechanistic": 0.2,
    "contexts": [
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Transformer Seq2Seq TS (Multitask) 29.8 32.2 - 738m TS (11b) (Roberts et al., 2020)",
      "Hawaii | 0.6",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB."
    ],
    "rationale": "Evidence mentions 2019 in relation to models and data sizes, but a direct causal link to '46.8 330' is missing."
  },
  {
    "node_i": 19,
    "node_j": 99,
    "node_i_label": "2019",
    "node_j_label": "110m TS",
    "support": 0.6,
    "temporal": 0.4,
    "mechanistic": 0.2,
    "contexts": [
      "Transformer Seq2Seq TS (Multitask) 27.0 29.1 - 223m TS (large) (Roberts et al., 2020)",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)"
    ],
    "rationale": "The evidence mentions 110m in 2019, but does not show a causal link."
  },
  {
    "node_i": 96,
    "node_j": 102,
    "node_i_label": "26.5",
    "node_j_label": "29.8",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Datastore 8 \u00a3 ro \u00a3 g fF - 36 Ayeaydiag Bj 17 u +34 - 32 Axaidag uyewop-uy 16 a a - 30 voneydepy 15 a - 28 ujewog f fF - 26 14 & 2 \u00b0 0.2 0.4 0.6 0.8 A (interpolation parameter)",
      "Hawaii | 0.6",
      "Illinois |0.2"
    ],
    "rationale": "The evidence shows some correlation between the entities, but lacks clear temporal or mechanistic details."
  },
  {
    "node_i": 96,
    "node_j": 99,
    "node_i_label": "26.5",
    "node_j_label": "110m TS",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Transformer Seq2Seq TS (Multitask) 27.0 29.1 - 223m TS (large) (Roberts et al., 2020)",
      "BERT 31.8 31.6 - 110m PathRetriever (Asai et al., 2019) PathRetriever+-Transformer MLM 32.6 - - 110m ORQA (Lee et al., 2019)",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)"
    ],
    "rationale": "The evidence mentions models with different sizes (26.5 is not explicitly mentioned, but close to 27.0), and 110m TS, suggesting a comparison or evolution, but the causal link is weak."
  },
  {
    "node_i": 115,
    "node_j": 126,
    "node_i_label": "31.6",
    "node_j_label": "CC-News",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:"
    ],
    "rationale": "Evidence suggests a weak link, possibly through retrieval probabilities, but lacks direct causal connection."
  },
  {
    "node_i": 115,
    "node_j": 123,
    "node_i_label": "31.6",
    "node_j_label": "Z Z",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.2,
    "contexts": [
      "d(z)",
      "Dense Retr.+Transformer ICT+BERT 33.3 36.4 30.1 330m Ours (\u00a5 = Wikipedia, Z Z = = Wikipedia) Dense Retr.+-Transformer REALM 39.2 40.2 46.8 330m Ours (\u00a5 = CC-News, Z = Wikipedia)",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "=encg(z), q(x) = = enc,(z)",
      "d(z)"
    ],
    "rationale": "The evidence mentions 'z' in different contexts, but a clear causal link to 'Z Z' is missing; some snippets show 'z' as a variable or part of a model."
  },
  {
    "node_i": 94,
    "node_j": 119,
    "node_i_label": "1k",
    "node_j_label": "ORQA",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "= (1- A)Pim =)",
      "+ APinn O12)",
      "o Before you start solving o For e.g., RAG, REALM etc."
    ],
    "rationale": "The evidence mentions '1k' in the context of pre-training architectures, but the connection to 'ORQA' is not explicitly stated, implying a weak causal link."
  },
  {
    "node_i": 92,
    "node_j": 119,
    "node_i_label": "NQ WQ",
    "node_j_label": "ORQA",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Pa 6",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "Transformer Seq2Seq TS (Multitask) 34.5 37.4 - 11318m DrQA (Chen et al., 2017)"
    ],
    "rationale": "The evidence mentions DrQA in relation to Transformer Seq2Seq, suggesting a possible connection, but the nature of the relationship isn't clear."
  },
  {
    "node_i": 19,
    "node_j": 119,
    "node_i_label": "2019",
    "node_j_label": "ORQA",
    "support": 0.6,
    "temporal": 0.7,
    "mechanistic": 0.2,
    "contexts": [
      "Pa 6",
      "oe NQ WQ cT Name Architectures Pre-training (79k/4k) (3k/2k) (1k /1k)",
      "# params BERT-Baseline (Lee et al., 2019)",
      "Ilinois | 0.2 Obama's birthplace is OOOO)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB."
    ],
    "rationale": "The evidence mentions '2019' in relation to a baseline model, but doesn't establish a clear causal link to 'ORQA'."
  },
  {
    "node_i": 4,
    "node_j": 123,
    "node_i_label": "al.",
    "node_j_label": "Z Z",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "d(z)",
      "=encg(z), q(x) = = enc,(z)",
      "d(z)",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "d(z)"
    ],
    "rationale": "The evidence mentions 'd(z)' multiple times, suggesting a relationship, but lacks clear causal direction or mechanism between 'al' and 'Z Z'."
  },
  {
    "node_i": 164,
    "node_j": 175,
    "node_i_label": "Aston Zhang",
    "node_j_label": "\u25aa Build a Large Language Model",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "# params BERT-Baseline (Lee et al., 2019)"
    ],
    "rationale": "Aston Zhang is a co-author of 'Dive into Deep Learning,' which provides foundational knowledge, but the direct causal link to building a specific LLM is weak."
  },
  {
    "node_i": 164,
    "node_j": 174,
    "node_i_label": "Aston Zhang",
    "node_j_label": "Thomas Wolf",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "Po | x\u00bb) x Zz \u20ac topk(py(-|X))",
      "Perplexity z 2 a 5 > \u00bb 18 \u00bb 17 o \u201c \u2014 9 --- << Wiki-3B Wiki-100M KNN-LM (Wiki-100M + kNN) || 14 0.0 0.5 1.0 15 2.0 25 3.0 Size of datastore (in billions)",
      "Because A yielded high reward, retriever embeddings adjust so that queries like x are closer to Doc A. o Over time, retriever learns to consistently fetch the most helpful documents."
    ],
    "rationale": "Both are authors in the field, but the evidence does not show a direct causal link."
  },
  {
    "node_i": 164,
    "node_j": 165,
    "node_i_label": "Aston Zhang",
    "node_j_label": "Zachary C. Lipton",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.2,
    "contexts": [
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "d(z)",
      "# params BERT-Baseline (Lee et al., 2019)",
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "Dy(z|x) x exp(d(z)'q(x))"
    ],
    "rationale": "The evidence mentions both entities as authors of a book, suggesting a collaboration but not necessarily a causal relationship."
  },
  {
    "node_i": 165,
    "node_j": 174,
    "node_i_label": "Zachary C. Lipton",
    "node_j_label": "Thomas Wolf",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.2,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "\u25aa Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola, Dive into Deep Learning, e-book, 2020.",
      "+ APinn O12)",
      "Obama is a native] of Hawaii)",
      "DR."
    ],
    "rationale": "Zachary C. Lipton and Thomas Wolf are co-authors on publications, suggesting a collaborative, but not necessarily causal, relationship."
  },
  {
    "node_i": 172,
    "node_j": 180,
    "node_i_label": "Lewis Tunstall",
    "node_j_label": "Tanmoy Chakraborty",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aa Lewis Tunstall, Leandro von Werra, and Thomas Wolf.",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "+ APinn O12)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o"
    ],
    "rationale": "The evidence mentions both Lewis Tunstall and Tanmoy Chakraborty in the context of publications, suggesting a possible collaboration or influence."
  },
  {
    "node_i": 172,
    "node_j": 179,
    "node_i_label": "Lewis Tunstall",
    "node_j_label": "\u25aa Introduction to Large Language Models",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.2,
    "contexts": [
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "NEAREST NEIGHBOR LANGUAGE MODELS, ICLR 2020",
      "\u25aa Introduction to Large Language Models \u2013 Generative AI for Text, Tanmoy Chakraborty, Wiley Publications, 2024 TULIKA SAHA, IIIT BANGALORE 3",
      "Advances in Natural Language Processing AID-849 MODULE INSTRUCTOR:",
      "Intermediate Fusion: RETRO TULIKA SAHA, IIIT BANGALORE 14 Credit: Improving language models by retrieving from trillions of tokens."
    ],
    "rationale": "Lewis Tunstall is not directly mentioned in the context, so the causal relationship to 'Introduction to Large Language Models' is weak."
  },
  {
    "node_i": 185,
    "node_j": 190,
    "node_i_label": "GPT",
    "node_j_label": "Mandate 3:",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "\u2014<\u2014 a Books 8 9 (In-domain) & 2 S \u00a3 3 = 18 - a = Wiki-3B (Gomain mn 2 + Adaptation} 3 Books 8 g",
      "Indexed Contexts (keys) ...",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2"
    ],
    "rationale": "GPT is mentioned as a pre-trained model, suggesting it is a component of the later 'Mandate 3' which discusses post-training strategies, but the connection is weak."
  },
  {
    "node_i": 185,
    "node_j": 186,
    "node_i_label": "GPT",
    "node_j_label": "Mandate 2:",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Course Contents Mandate 1: Pretraining & Pre-trained LLMs \u2022 Overview of pre-training: Pre-training objectives, Pre-training data \u2022 Representative pre-trained models such as BERT, GPT etc. Mandate 2: Advanced Post-training Strategies-I \u2022 Multi-tasking \u2022 Fine-tuning and Instruction Tuning, Parameter Efficient Fine-tuning Mandate 3: Advanced Post-training Strategies-II \u2022 In-context Learning:",
      "Transformer Seq2Seq TS (Multitask) 27.0 29.1 - 223m TS (large) (Roberts et al., 2020)",
      "A :hyperparameter Piyn-tmQ12) |x)",
      "Administrative Trivia Classroom: R102 Lecture slot: \u25e6Monday 2:00 - 3:30PM \u25e6Wednesday 2:00 \u2013 3:30PM Evaluation Scheme \u25e6Exam & Quiz: 30% \u25e6Assignments: 20% \u25e6Project: 40% \u25e6Attendance: 10% TULIKA SAHA, IIIT BANGALORE 2",
      "Indexed Contexts (keys) ..."
    ],
    "rationale": "GPT is mentioned as a pre-trained model, suggesting it is a component of or related to the broader topic of Mandate 2, which covers post-training strategies."
  },
  {
    "node_i": 195,
    "node_j": 199,
    "node_i_label": "Models \u2022 Multi-modal",
    "node_j_label": "the Loop Feedback",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "Prompting Methods, Multi-prompt Learning, Prompt- aware Training Methods Mandate 4: Retrieval & RAG \u2022 Retrieval Methods, Retrieval Augmented Generation Mandate 5: Distillation, Quantization, and Pruning \u2022 Distillation, Quantization, Pruning Mandate 6: Advanced Models \u2022 Mixture of Experts Model, Agentic AI Mandate 7: Multi-modal Models \u2022 Multi-modal LLMs \u2022 Vision-Language Models Mandate 8: Reinforcement Learning for NLP \u2022 Policy Learning using DQN \u2022 Reinforcement Learning with Human in the Loop Feedback (RLHF) TULIKA SAHA, IIIT BANGALORE 4",
      "\u25aa Daniel Graupe, Deep Learning Neural Networks: Design and Case Studies, World Scientific Publishing Co., Inc., 2016.",
      "Output Probabilities Add & Norm Feed Forward Add & Norm Add & Norm Multi-Head Attention Nx Masked Multi-Head Multi-Head Attention Attention \\ 4 Ae \u2018ositional @",
      "\u25aa Build a Large Language Model from scratch by Sebastian Raschka, Manning Publications, 2025.",
      "Output Interpolations: kNN-LMs TULIKA SAHA, IIIT BANGALORE 13 \u25aaLimitations:"
    ],
    "rationale": "The evidence mentions multi-modal models and reinforcement learning with human feedback, suggesting a potential link, but lacks explicit causal mechanisms."
  },
  {
    "node_i": 86,
    "node_j": 89,
    "node_i_label": "95%",
    "node_j_label": "\u25aaRetriever",
    "support": 0.6,
    "temporal": 0.5,
    "mechanistic": 0.3,
    "contexts": [
      "\u25aaRetriever gets gradient signal: \u201cincrease probability of picking A, decrease for C.\u201d o",
      "o If Doc C retrieved \u2192 [MASK] with 5% confidence --> R(C) = log(0.05)",
      "17.6 fe te KNN-LM on Wikitext-103 174 fe 17.2 fev Perplexity UB.",
      "Aggregation = = Se S\u2014 lyenpth)",
      "16.8 fever fevvneereeoneeeeeeee? 3.9 Poe C8 2 a ROO coe Py \u00bb a N a aa 1 2 8 64 256 1024 k (# nearest neighbors)"
    ],
    "rationale": "The retriever's gradient signal influences the probability, suggesting a causal link, but the evidence is weak."
  }
]